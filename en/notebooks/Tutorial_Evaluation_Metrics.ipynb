{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tutorial_Evaluation_Metrics.ipynb","provenance":[{"file_id":"1I6xkbkPSnEZh3g9XINTiyCRFfoPVADi-","timestamp":1602518805470}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_3FtGybCw0W0"},"source":["# Evaluation Metrics\n"]},{"cell_type":"markdown","metadata":{"id":"kU1eSeD767p0"},"source":["Evaluation Metrics is a way to **quantify performance** of a Machine Learning model,  there are different metrics for the tasks of classification, regression, clustering, etc. On this Notebook we will focus on Supervised Learning Techniques Evaluations.\n"]},{"cell_type":"markdown","metadata":{"id":"KPoN957z6jGR"},"source":["## Classification Problems\n"]},{"cell_type":"markdown","metadata":{"id":"D0PlCVYIZ3_z"},"source":["In order to demonstrate some metrics from scikit-learn we're going to use the digits dataset, and build a digit 9 classifier."]},{"cell_type":"code","metadata":{"id":"KdAikOvpaQC2","executionInfo":{"status":"ok","timestamp":1602621321961,"user_tz":180,"elapsed":1356,"user":{"displayName":"Gabriela Cavalcante da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVfiOJ9jqHVGSXifuJFJhIUKjpjedrtg4OoCbriQ=s64","userId":"08707813586532185234"}}},"source":["import numpy as np\n","import pandas as pd\n","from sklearn import datasets\n","\n","digits = datasets.load_digits()\n","X = digits.data\n","y = digits.target.copy()\n","\n","y[digits.target == 9] = 1 # Set 1 where the digit is \"9\"\n","y[digits.target != 9] = 0 # Set 0 otherwise"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T1AEu2OAae3N"},"source":["Now we split the dataset into train and test, and run a linear model."]},{"cell_type":"code","metadata":{"id":"q2cYs2Zmaj4z","executionInfo":{"status":"ok","timestamp":1602621555570,"user_tz":180,"elapsed":1284,"user":{"displayName":"Gabriela Cavalcante da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVfiOJ9jqHVGSXifuJFJhIUKjpjedrtg4OoCbriQ=s64","userId":"08707813586532185234"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 6)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"nSMUcvAcavyR","executionInfo":{"status":"ok","timestamp":1602621556813,"user_tz":180,"elapsed":1387,"user":{"displayName":"Gabriela Cavalcante da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVfiOJ9jqHVGSXifuJFJhIUKjpjedrtg4OoCbriQ=s64","userId":"08707813586532185234"}},"outputId":"96e6bc07-e863-4657-95fe-064cff105b7e","colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["from sklearn.linear_model import LogisticRegression\n","log_reg = LogisticRegression()\n","log_reg.fit(X_train,y_train) # Run the algorithm\n","\n","# Run prediction on X test dataset for further analysis \n","y_log_predict = log_reg.predict(X_test)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"kWxsdgZH6xmo"},"source":["### Model Accuracy :  \n","  Classification Accuracy shows how many of the predictions are correct and can be obtained the following formula:   \n","  &nbsp;   \n","\\begin{equation*}\n","Accuracy = \\frac{Correct Predictions}{Total Predictions}\n","\\end{equation*}    \n","  "]},{"cell_type":"code","metadata":{"id":"ORJi3VorbMCY","executionInfo":{"status":"ok","timestamp":1602621753121,"user_tz":180,"elapsed":1017,"user":{"displayName":"Gabriela Cavalcante da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVfiOJ9jqHVGSXifuJFJhIUKjpjedrtg4OoCbriQ=s64","userId":"08707813586532185234"}},"outputId":"8e967b80-3e6d-4869-eb0d-c89394873f54","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Run model accuracy passing the test X and y datasets\n","log_reg.score(X_test, y_test)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9844444444444445"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"cAUIbGC_bXxE"},"source":["Our example got a 98% accuracy. Now, let's see other metrics on the same model."]},{"cell_type":"markdown","metadata":{"id":"8V8LOQV8VYhv"},"source":["\n","### Precision and Recall :\n","  In some cases, just the accuracy is not enought to measure how good a model generalize tha data, this became evident on non well balanced datasets. To solve this problem was created the following metrics.   To Fully evaluate the efectiveness of a model, you need examine both precision and recall. Unfortunately, improving precision typically reduces recall and vice versa.\n","  * Precision calculates the true positives divided by total positive predictions, this measure how precise our model is  out of those predicted positive. Is a good metric to determine, when the costs of False Positive is high.\n","  * Recall calculates the true positives divided by all positive cases, so recall is a good metric when there is a high cost associated with False Negative.\n","  \n","    \\begin{equation*}\n","      Precision = \\frac{TruePositive}{TruePositive + FalsePositive}\n","    \\end{equation*} \n"," &nbsp;\n","    \\begin{equation*}\n","      Recall = \\frac{TruePositive}{TruePositive + FalseNegative}\n","    \\end{equation*} \n","\n","&nbsp;\n","```\n","True Positive:          When the prediction is correct and the valor is positive\n","True Negative           When the prediction is correct and the valor is negative\n","False Positive:         When the prediction is wrong and the valor is positive\n","False Negative:         When the prediction is wrong and the valor is negative\n","```\n"]},{"cell_type":"code","metadata":{"id":"4SQztnWwbnhd","executionInfo":{"status":"ok","timestamp":1602622159722,"user_tz":180,"elapsed":844,"user":{"displayName":"Gabriela Cavalcante da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVfiOJ9jqHVGSXifuJFJhIUKjpjedrtg4OoCbriQ=s64","userId":"08707813586532185234"}},"outputId":"502f89c3-9bd4-4cb1-f44b-a0fccbc03f2e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.metrics import precision_score\n","\n","precision_score(y_test, y_log_predict)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8888888888888888"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"NA_q4cPVbtDx","executionInfo":{"status":"ok","timestamp":1602622160086,"user_tz":180,"elapsed":616,"user":{"displayName":"Gabriela Cavalcante da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVfiOJ9jqHVGSXifuJFJhIUKjpjedrtg4OoCbriQ=s64","userId":"08707813586532185234"}},"outputId":"e06cbf94-c63a-4584-a39f-b52806f13f6c","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.metrics import recall_score\n","\n","recall_score(y_test, y_log_predict)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9142857142857143"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"vT6RrneRVeZb"},"source":["   \n","### Confusion Matrix : \n","This is a common technique used to easily see all the metrics above, showing  all the corrects and wrongs predictions (True Positive, True Negative, False Positive, False Negative).     \n","![image.png](https://miro.medium.com/max/356/1*g5zpskPaxO8uSl0OWT4NTQ.png)\n","\n"]},{"cell_type":"code","metadata":{"id":"Oi6u0ZwebwuB","executionInfo":{"status":"ok","timestamp":1602622161513,"user_tz":180,"elapsed":805,"user":{"displayName":"Gabriela Cavalcante da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVfiOJ9jqHVGSXifuJFJhIUKjpjedrtg4OoCbriQ=s64","userId":"08707813586532185234"}},"outputId":"c2c56065-e355-42b2-a832-42cfa2644e1a","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from sklearn.metrics import confusion_matrix\n","\n","confusion_matrix(y_test, y_log_predict)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[411,   4],\n","       [  3,  32]])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"NXYx28l4VhGZ"},"source":["\n","### Area under the ROC Curve:\n","Receiver Operating Characteristics is a probability curve and AUC (Area Under the Curve) represents a degree of measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s.   An excellent model has AUC near to the 1 which means it has good measure of separability. A poor model has AUC near to the 0 which means it has worst measure of separability.\n","* TPR (True Positive Rate) / Recall  &nbsp;\n","\\begin{equation*}\n","      TPR = \\frac{TruePositive}{TruePositive + FalseNegative}\n","\\end{equation*} \n","* Specificity &nbsp;\n","\\begin{equation*}\n","      Specificity = \\frac{TrueNegative}{TrueNegative + FalsePositive}\n","\\end{equation*}\n","* FPR   &nbsp;\n","\\begin{equation*}\n","      FPR = 1 - Specificity\n","\\end{equation*} \n","&nbsp;\n","\\begin{equation*}\n","      FPR = \\frac{FalsePositive}{TrueNegative + FalsePositive}\n","\\end{equation*} \n","\n","    ![image.png](https://miro.medium.com/max/361/1*pk05QGzoWhCgRiiFbz-oKQ.png)"]},{"cell_type":"code","metadata":{"id":"d4E6AoF0cfaP","executionInfo":{"status":"ok","timestamp":1602622242081,"user_tz":180,"elapsed":765,"user":{"displayName":"Gabriela Cavalcante da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVfiOJ9jqHVGSXifuJFJhIUKjpjedrtg4OoCbriQ=s64","userId":"08707813586532185234"}},"outputId":"839adc70-0b64-411c-b604-081dca6ce9f3","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["descision_score = log_reg.predict_proba(X_test)[:,1]\n","\n","from sklearn.metrics import roc_auc_score\n","roc_auc_score(y_test,descision_score)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9825817555938039"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"7N3UX2aU6t8x"},"source":["## Regression Problems"]},{"cell_type":"markdown","metadata":{"id":"zJbddPar-asC"},"source":["Regression problems are those where you want to predict a continuous value. For example, predicting the selling price of a house.  To study this topic we are going to use [the Boston house-prices dataset](https://scikit-learn.org/stable/datasets/index.html#boston-dataset). In this example we can use the sklearn.datasets module, that includes utilities to load and fetch popular reference datasets."]},{"cell_type":"markdown","metadata":{"id":"WLKVorQDUTiE"},"source":["### Preparing the data"]},{"cell_type":"markdown","metadata":{"id":"jQJ2sLxWUWn_"},"source":["\n","First we are going to i take a look on some information about the dataset."]},{"cell_type":"code","metadata":{"id":"lR7Up1BbnQxZ","executionInfo":{"status":"ok","timestamp":1602622263331,"user_tz":180,"elapsed":967,"user":{"displayName":"Gabriela Cavalcante da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVfiOJ9jqHVGSXifuJFJhIUKjpjedrtg4OoCbriQ=s64","userId":"08707813586532185234"}},"outputId":"febfe652-ef8c-451b-b794-cc8b22d59144","colab":{"base_uri":"https://localhost:8080/","height":904}},"source":["from sklearn.datasets import load_boston\n","\n","boston = load_boston()\n","print(boston.DESCR)"],"execution_count":14,"outputs":[{"output_type":"stream","text":[".. _boston_dataset:\n","\n","Boston house prices dataset\n","---------------------------\n","\n","**Data Set Characteristics:**  \n","\n","    :Number of Instances: 506 \n","\n","    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n","\n","    :Attribute Information (in order):\n","        - CRIM     per capita crime rate by town\n","        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n","        - INDUS    proportion of non-retail business acres per town\n","        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n","        - NOX      nitric oxides concentration (parts per 10 million)\n","        - RM       average number of rooms per dwelling\n","        - AGE      proportion of owner-occupied units built prior to 1940\n","        - DIS      weighted distances to five Boston employment centres\n","        - RAD      index of accessibility to radial highways\n","        - TAX      full-value property-tax rate per $10,000\n","        - PTRATIO  pupil-teacher ratio by town\n","        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n","        - LSTAT    % lower status of the population\n","        - MEDV     Median value of owner-occupied homes in $1000's\n","\n","    :Missing Attribute Values: None\n","\n","    :Creator: Harrison, D. and Rubinfeld, D.L.\n","\n","This is a copy of UCI ML housing dataset.\n","https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n","\n","\n","This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n","\n","The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n","prices and the demand for clean air', J. Environ. Economics & Management,\n","vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n","...', Wiley, 1980.   N.B. Various transformations are used in the table on\n","pages 244-261 of the latter.\n","\n","The Boston house-price data has been used in many machine learning papers that address regression\n","problems.   \n","     \n",".. topic:: References\n","\n","   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n","   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jDpQJDJ8UZz9"},"source":["Now we load the data and include the target variable to the dataframe."]},{"cell_type":"code","metadata":{"id":"4oOSu1HtnVj3","executionInfo":{"status":"ok","timestamp":1602622270210,"user_tz":180,"elapsed":914,"user":{"displayName":"Gabriela Cavalcante da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVfiOJ9jqHVGSXifuJFJhIUKjpjedrtg4OoCbriQ=s64","userId":"08707813586532185234"}},"outputId":"25322721-4a10-476b-f4be-d428a13f8f82","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# Converting to dataframe\n","dataset = pd.DataFrame(boston.data, columns = boston.feature_names)\n","\n","# Adding the target variable to the dataframe\n","dataset['PRICE'] = boston.target\n","dataset.head()"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CRIM</th>\n","      <th>ZN</th>\n","      <th>INDUS</th>\n","      <th>CHAS</th>\n","      <th>NOX</th>\n","      <th>RM</th>\n","      <th>AGE</th>\n","      <th>DIS</th>\n","      <th>RAD</th>\n","      <th>TAX</th>\n","      <th>PTRATIO</th>\n","      <th>B</th>\n","      <th>LSTAT</th>\n","      <th>PRICE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00632</td>\n","      <td>18.0</td>\n","      <td>2.31</td>\n","      <td>0.0</td>\n","      <td>0.538</td>\n","      <td>6.575</td>\n","      <td>65.2</td>\n","      <td>4.0900</td>\n","      <td>1.0</td>\n","      <td>296.0</td>\n","      <td>15.3</td>\n","      <td>396.90</td>\n","      <td>4.98</td>\n","      <td>24.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.02731</td>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0.0</td>\n","      <td>0.469</td>\n","      <td>6.421</td>\n","      <td>78.9</td>\n","      <td>4.9671</td>\n","      <td>2.0</td>\n","      <td>242.0</td>\n","      <td>17.8</td>\n","      <td>396.90</td>\n","      <td>9.14</td>\n","      <td>21.6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.02729</td>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0.0</td>\n","      <td>0.469</td>\n","      <td>7.185</td>\n","      <td>61.1</td>\n","      <td>4.9671</td>\n","      <td>2.0</td>\n","      <td>242.0</td>\n","      <td>17.8</td>\n","      <td>392.83</td>\n","      <td>4.03</td>\n","      <td>34.7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.03237</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0.0</td>\n","      <td>0.458</td>\n","      <td>6.998</td>\n","      <td>45.8</td>\n","      <td>6.0622</td>\n","      <td>3.0</td>\n","      <td>222.0</td>\n","      <td>18.7</td>\n","      <td>394.63</td>\n","      <td>2.94</td>\n","      <td>33.4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.06905</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0.0</td>\n","      <td>0.458</td>\n","      <td>7.147</td>\n","      <td>54.2</td>\n","      <td>6.0622</td>\n","      <td>3.0</td>\n","      <td>222.0</td>\n","      <td>18.7</td>\n","      <td>396.90</td>\n","      <td>5.33</td>\n","      <td>36.2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      CRIM    ZN  INDUS  CHAS    NOX  ...    TAX  PTRATIO       B  LSTAT  PRICE\n","0  0.00632  18.0   2.31   0.0  0.538  ...  296.0     15.3  396.90   4.98   24.0\n","1  0.02731   0.0   7.07   0.0  0.469  ...  242.0     17.8  396.90   9.14   21.6\n","2  0.02729   0.0   7.07   0.0  0.469  ...  242.0     17.8  392.83   4.03   34.7\n","3  0.03237   0.0   2.18   0.0  0.458  ...  222.0     18.7  394.63   2.94   33.4\n","4  0.06905   0.0   2.18   0.0  0.458  ...  222.0     18.7  396.90   5.33   36.2\n","\n","[5 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"4Ed9rlrpUxZD"},"source":["To simplify, our goal will be to study the regression metrics creating a machine learning model using only the RM(average number of rooms per dwelling) and PRICE columns. Understanding the steps you can try to apply it to all features."]},{"cell_type":"code","metadata":{"id":"5quQgKwnnXls","executionInfo":{"status":"ok","timestamp":1602622275362,"user_tz":180,"elapsed":856,"user":{"displayName":"Gabriela Cavalcante da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVfiOJ9jqHVGSXifuJFJhIUKjpjedrtg4OoCbriQ=s64","userId":"08707813586532185234"}}},"source":["# Reshaping\n","X = np.array(dataset['RM']).reshape(-1,1)\n","Y = np.array(dataset['PRICE']).reshape(-1,1)\n","\n","# Splitting the data\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size= 0.4, random_state=11)\n","\n","# Training the model\n","from sklearn.linear_model import LinearRegression\n","lm = LinearRegression()\n","lm.fit(X_train, y_train)\n","\n","# Getting predictions\n","predictions = lm.predict(X_test)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e0yIJBzq_YJS"},"source":["### Mean Absolute Error (MAE)\n","This is the simplest of all the metrics. It is measured by taking the average of the absolute difference between actual values and the predictions. It indicater us the difference between the predictions and the actual output. However, they don’t gives us any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data. The less the value of MAE the better the performance of your model.\n","![image](https://i.imgur.com/BmBC8VW.jpg)"]},{"cell_type":"code","metadata":{"id":"G9N0rNpUnlZe","executionInfo":{"status":"ok","timestamp":1602616428505,"user_tz":180,"elapsed":705,"user":{"displayName":"Gabriel Igor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gie_qCEqpeOr4IkotFDgNsAmdFLG-g_-eKWhZSfsQ=s64","userId":"00351748451610211730"}},"outputId":"5c3ffbfd-504d-4ba6-8477-36a0e2da7a6f","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.metrics import mean_absolute_error\n","print('MAE:', mean_absolute_error(y_test, predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MAE: 4.488594176693228\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ShtddJSZ_bmt"},"source":["### Mean Squared Error (MSE)"]},{"cell_type":"markdown","metadata":{"id":"5H_P7ftYDh-u"},"source":["MSE is similar to MAE, but the difference is that it squares the difference between actual and predicted output values ​​before summing them all instead of using the absolute value. As, we take square of the error, the effect of larger errors become more pronounced then smaller error, hence the model can now focus more on the larger errors. In this metric, the larger the number the larger the error.\n","\n","![image](https://cdn-media-1.freecodecamp.org/images/hmZydSW9YegiMVPWq2JBpOpai3CejzQpGkNG)"]},{"cell_type":"code","metadata":{"id":"olCW25LsnnJL","executionInfo":{"status":"ok","timestamp":1602616440356,"user_tz":180,"elapsed":784,"user":{"displayName":"Gabriel Igor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gie_qCEqpeOr4IkotFDgNsAmdFLG-g_-eKWhZSfsQ=s64","userId":"00351748451610211730"}},"outputId":"df3c0612-8737-410e-dc03-edf278c0d3ae","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.metrics import mean_squared_error\n","print('MSE:', mean_squared_error(y_test, predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MSE: 43.60590161966949\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d-zjXkRgCibc"},"source":["### R2 Score"]},{"cell_type":"markdown","metadata":{"id":"z1pmDtUZHRH6"},"source":["R2, also known as the coefficient determination, defines the degree to which the variance in the dependent  target can be explained by the features.If the R2 value for a particular model is 0.7, this means that 70% of the variation  in the dependent target can be explained by the variation in the features.    \n","\n","  "]},{"cell_type":"markdown","metadata":{"id":"SMwlftPNQ1nd"},"source":["* **SS ERROR**: **sum of squares of residuals**  is the sum of the squares of residuals (deviations predicted from actual empirical values of data). It is a measure of the discrepancy between the data and an estimation model.  \n","$\\sum (y_{i} - ỹ)^2$\n","* **SS TOTAL**: **sum of squares of errors** is the squared differences between the observed dependent variable and its mean. You can think of this as the dispersion of the observed variables around the mean – much like the variance in descriptive statistics.    \n","$\\sum(y_{i} - ŷ_{i})^2$\n","\n","* **R 2**:  **R squared** is calculated by dividing the **SS ERROR**  by **SS TOTAL** and then subtract it from 1.\n","\n","\\begin{equation*}\n","      R^2 = 1 - \\frac{\\sum (y_{i} - ỹ)^2}{\\sum(y_{i} - ŷ_{i})^2}\n","\\end{equation*} "]},{"cell_type":"code","metadata":{"id":"R7jLsq1VnpFs","executionInfo":{"status":"ok","timestamp":1602616448010,"user_tz":180,"elapsed":711,"user":{"displayName":"Gabriel Igor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gie_qCEqpeOr4IkotFDgNsAmdFLG-g_-eKWhZSfsQ=s64","userId":"00351748451610211730"}},"outputId":"29ac1274-c5fd-42bf-e928-53b9072957d0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.metrics import r2_score\n","print('R2:', r2_score(y_test, predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["R2: 0.48350092178106796\n"],"name":"stdout"}]}]}