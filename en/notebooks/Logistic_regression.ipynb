{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_regression",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFaQFqfHES3o"
      },
      "source": [
        "# **Generalized linear models**: Logistic regression\n",
        "\n",
        "Logistic regression is a <ins>supervised machine learning technique</ins>, such as linear regression. However, we will be dealing with a predictive model for problems where the dependent variable is neither ordered or continuous. In other words, logistic regression adapts to problems that contain discrete values ​​in their columns (as well as boolean/binary variables).\n",
        "\n",
        "These discrete values ​​can be presented in different ways. Some examples are:\n",
        "- **Conditional:**  1 or 0; True or False; Accept or Don't Accept; Pass or Fail.\n",
        "- **Multiclass:** Blood types (A, B, AB, O); Alternatives of an objective exam (A, B, C, D, E).\n",
        "\n",
        "When dealing with generalized linear models, **logistic regression** submits us to a <ins>classification problem</ins>. In this type of classification, we will have a finite set of possible values ​​that represent different categories. In most cases, these categories are transformed into numeric variables (integers), which greatly facilitates code implementation and data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGLLhaU1Oe2U"
      },
      "source": [
        "While a linear regression model generates a real number as a classification parameter, logistic regression generates a probability value as the main parameter, called *threshold probability*. For example: in the binary classification, if the probability value is greater than a certain probability limit, we assign the label for that line to 1 or 0 otherwise, as shown in the next figure.\n",
        "\n",
        "![logistic-regression-in-machine-learning.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAfQAAAEsCAMAAAACZbH6AAABC1BMVEUAAAAAAAB/f3833QDRBQXtHCS/v7/dRERAQEAFJeLogoLzwMD87+/54ODXJCTUFRXuoaH20NDiY2P+8fGfn5/f39/aNDTlc3PrkZHuKjLxsbH4qq3Z+c+08p/wR03v7+/MAgLN9r/fU1PvOD/6xsjzY2n94+T81Nb3m5926FARDw8gICD0cXb2jZFgYGBwcHDyVVtp5kCvr68xMDD1gIT5uLrPz89Z1jDsoKDZQUHSIiJQUFCPj4+C6mDPEhJFXOjAyPiyFRuhrfS5hohZb+zicXHWMjLQ1vp+EhZOCQxlrQncUVHBFx3M4atOIqfDJU5nySSddb7KgajGVn+WH2t9VVenTFDeGiKjExmsOswvAAAAAXRSTlMAQObYZgAAEIZJREFUeNrs2MFOwkAQgOEZMHWDEIUs8aJpPdombaKJqdV49P0fyboUAlIuXCQz/3ecHv9Mu10BAAAA4MGy6CTpihuBaTHKYFW00muLlcC2EGSw2Kx69zUXmLZWXctgVrS/iz4T2NaoNjKYf3UiXSewLS9Vy3xv1VtOceZF7UXZ6r67d4FxlfYq2WqLYiGwLWoSZatg0c0LmgSi+5HpICO6G0EHgeheZLqTEd2JoDuB6D5E3RMluVkKLKt0TyVwYKIHJgLz8lIPlLnAulr/qAXGZXokE5iWV3qk4gVvW6MjGoFhmW5UTa1aNxUvePs2J/eyfhGZpJ+1l7rkBG9cUNWPmAqn6L08fqTbWBjV6OvnWpIherL+fOWzbtVb8ybJNvrhI5g34QLWH6I7RHSHiO4Q0R0iukNEd4joDhHdIaI7RHSHiO4Q0R0iukNEd4joDhHdIaI7RHSHiO4Q0R0iuguL2R3RfZnfX11dE92X1e2M6N4s50L0Ux6ep73nh9Hh+aNLQPQTHqeDx9Hh+aMLQPRxT9Odp9Hh+aP/98PevfU2DYNhHH8ybMxycg40SVvUST1s1aBjwICLgTgIiYM4ihu+/yfBdQLJIKUNEiL1658Eld7Bzf6y42apZqO3u1dnutc6/PvR/2ejt7vW0Dr8+9H/93+iH13vu2am1uG81BwdlLaNOoLyb6Nbb9+++fbiBW9mesErzWHLyCltG3U0MCb6jT568PbNe155V1d6x1uGn89Ln+vR44PS4y2jjtbRjdjeeyhdLvjaZLEaz+fN8/aHeWpP7yYaL3Tw1Ty179NpSFczVXwxTltuvj29tWXYbdSPO3JeTD16ejHkfLY6BlEEo6crlfz8IeiiF325Tj4HZdSin0w4n9BOTi16esH5cAnqSEVfL/NFCvIoRV9yPqR8fiMYPV3YZU4t+nprt1dzWtEfDvnsBBal6GPOJ3ZrpxX9gvM7+KMjSr/JhET0O5yvsCX6gwHIoBBdNR9jm8vT+6CCQPSdmuOm49ylssWbH10338Gp47Qvdt/zfDTFnoe9Znz0XZvjzFFuXP8tecKUUT1ww3Kwx0yPvnNzXHe0swGuSKQHjAL8IKQbAHGAPWZ49KVuvpvbTlt2maPJZTH2ntnRx5wvu3wrKpeNTZ4laPCli0oUQXE9IPLiaRS7ARR3BMSJSHq9Exgd/aS+JzM42OrMqd2+eYRSwgrURvVCFwIKcwGRyywKWK6/Hqk/iStkn6ubHP14yBeoPHI6ukTJFyx0/Xp3R0t0WQCYhvoEAOjNIEzQXwZHTyfN++2dVvpp80MmkWCZj8BTNkQPoUQs1icAjwX6jI/+Mjj6gg+P0cFZvbfjqhGbqtwKCha0RBfVkU/v/i4rob/Mjb7kfI4uTluTazlDKWbRxuiJRJJBRfeUXt+/MTb6vOszE/c3Jm9eyrPQR0lkAPxG9Jh5MgJG/X9TZ2r0VB/iOu/uB0f4RQQgkAkqMcsCKD6KddykER1hJn394ut/0F+mRl/wWYoujlTyGwP8ymdS6IPcD17IMiFkBF8yESaiEb1gCZRYSiHCCP1laPQl5yfo5KZz+gQtgsh1PTR5bjnxC/USB0AcQ/M9v3xV/6XPl3RDo590fwjyktBTFMZFf/3qUPn4Fd0Mzqj8MN3A6C8PKy9hEYn+/PCn57BoRH9WR38Gi0b0wwZYNrploxNmWHR7TScY3Z7eCUbHF/s+nVz0Y/5J7/CvXsOiEv2cn8OiFX3O+TF2EwkRoeIXQhR9/gG4jf4HE36B3SQyT5gLLZChm8uMTnWjoo/5MMVOPDYCXBZgLQ/99aTPjz3Y6BvN+Aq/G7k+gMCNUUtCAD4rrj7iSIVJ0Vd8hhaRXsM5C1DLkvpvuMwDCtbnz6TY6JufhRyjTSgAhFPkohJVy1oIaFM2TeRef/iYbPQNC728dscsQuxVgl+iu1JISeeSblD0jQsdASuQSzTV0XVz/TBzr59ltNE7LHRtGiLM0VzpYgpF6vT6a3r/p8KY6JsXuv4AMYvRvKbnod4BRleXPRXGRNcLfZOQZb/t+HoDQPCdvbPraRuGwvAbZNfM+Q4jSTOJSS2FinUMDdjNuJimSbufpv3/n7LYTWn6QVcG+7DPeSTKSV1HrR75xHYSZxRilMY8TneRXQ0dqDaM1qJM0tz8HyFOhFLif76hHGDpWyfjxjsnXbFOrusYQGRfG63/+7sOWfoG410NPRQ1GO+k72rokRZ0OuaUpJ/Itw9LFxWdE2iEpJ8+fHrtP79VfD9Y+hYm8gYMLelnUpJ9bipZ6TM5AUNL+tFQUn+QJj3pZrzGEJNuJmYYWtJP5ZCfvkVN+kzOwNCSfiQlP1qRmvSpPAFDTDp34+hJP+duHD3p3I0jKH3I3Thy0nk2jqD0iXwLhpb0Iz6pSk86D9IJSj/hQTo56WdS8iCdmvQpXzJDTzpnd3rSObsTlM7ZnaB0zu70pHN2f4iwUEW43NIWP6Rzdn+ATCQqFRk6tLB4IP39tXkc7nswG8RCRYiSNMIcpTxJ7weBwdEv/4cpRAygEbVn0t8HC7itb5CWMIiFa6H9kH4dLLgGs0okChjKZLGtdBavST/sIsvx3vFLGw/2jgc2frl3fHxg2R4HS9otB3/Alhj2mz2dsGvZSiy2DUW0emyEJbAM9o4PbXywdzwXdbh3PAgsW+M+rv6AjRjmmz2/dEtTioql//sfsDXGH5KOKBHOp/c3wYI33qR3Ez/nMT1R6FGJ0PWOHPfed5Akffcd2n3pPE5/mK5Ra5EBUYSOJHV+yAb8uDDjNW7nW4jTMkaYliZKY9R5G4xE7YH0IV8G+yBZKoRIciAXIkcqDNr9yRmcyjE6ro/BrBLXOpvrz+Zn2bLYgxk53MgZOt5ccJKncT79RN7CYsfS716D8V56//qJd6ZH9wGM79Jfyau10dsBN3bfpU/kFAtetsr5ggoC0vt3pQ+CjkPO8T5LtwO2vvSOSx69+Sv9pr/U94egx+UAjJ/Sx/IWS4IVru/AeCj9TEr06Bv/eDDgbryX0qdyskX6xztO7R5Ln8jphvSL4BKMv9LXF5qxw7VBEHBD91j6rTzBmvTr17jkq6F9lj6TN+hx3Ob2D8Dri+AlGF+lry0SOegulLsLLrjj7qt0Mx23Kv0OluvgHRg/pZvrJ/ocXC6bPM++eyrdTMf1eY8FH4NDMD5KP5dDPMAx9+U8lb5r/YkD7sv5KX3n6kJvXPs1e0Jc+u7VhQZBwKfU/ZO+PbtzX85r6TuzO3DMtzT6J/2Xa8cdrDR1HWKT3eWxjveqnWVwFdek/3LtuNd3gFgAobGLbeWhCDdLywSWpMQCpeAqrknfa2XQMAxrUbevzya9EjFaclGz9L/N7uy+qe7ZpFvb1j1L/9vY7P446XmhqgiAjhuVA81IabMZm9VT++WIKzWq72uG5m0rvZ/XE2VLRk0nPdTLo3utlCtHecekm+z+KOlloqu0tPqFylGllTb+4lTpou6XN6kyUTSvmYm2POlJr0QE5CJD1O5BiWYuXYv7Nm93VMEJ3JJusvvjpCcRkBlHIs2NtRCIRYZaRFgtT0cw5drWjOxWvZRufHfmTUVVrkuvU7ujGC7glvQbOcEj0/viVRQAdGIlFchEjZXyRuQmGpWmZrfVT+9IRvbPEIWFWJeuCrsjNxK8W9LH8vZ3pdtICYsCCpHqfrnVZyzamt1WX3olkFunTZKqZEO6mKPhAk5JP5VDPE16GRpyAHkhqsdIz0VTp8a5Ma83pRehgdP7szOTs6dJr1IsKdJeeSgaE5WqS++mdtyXjqRSRWe4Jz2aV0I5gjs4JX0oz58mPRbGW9QGAEZJvzxJIiOxmdc03fholPalF6VobK0IcWqld8P3TCjzktk9O4FL0s1js58m3T7oQqUxilSVIuuX54l5q+5qNmmqkkL1pTcine84aUs66VCiLMvKRIUolUrgBC5Jv5JT7EsURlZR3L12EeJa1zkQZbqOV8qBxr5la9rPhchj9AhzGOL2Y+Yzud3MdIbYRrnWmRuHdJekn/Fjs+lJf8sPZ6In3QzSGVrSb+UYDDHpV/ItGFrSuRtHUPoNd+PIST8aylMwtKRPuRtHT/qYH5VOTvorOQRDTDqP1+hJP5XDIzC0pF/JGRha0k95YoaedG7o9KRzQyconRs6Penc0EFPOjd0kJPODR0gJ33Mk3GgJn3Kk3EAMelHQzkFQ0v6DZ9Hb6El/VzyBTMttKRf8ZVxBlLSp3LIw7UWStLPuBc3h5L0iTwB00JI+q2U52Ba6Eg/GvJcXAcd6Zzc7/Ff+tcvL1q+fOPkfo/30j+/6Pi+Z3LPlouyRtlIFSEYx6R/enHPJ+xDkVaF0J3/pNKlaMC4Jf1ne/fa1DQQhmH4iewSzDltTQ9qulKxCkJRGWxR66gDnvWT4///J2ZLSpeSBIlkaNb3mqHMvMO3e7o5kG7fLqK/xV+wZWOLuZAc+ePVaUu3v6V39NsKLOmd7tpthVhoeDI06+CMSdF1ih6wIN1pfyFqpK9zAS3vWkWHZ8qXTbTNVABmqV+zEJp0SK9h9MJjujx2hyxAaKfcpeiu1fa8EKRe0QvP3l3WQduHQo2eqstmnVehefTFdfp7XLTpwWtDeaenp22+pW7iSlfqdYs+/PljdkfuIzL0WMBCqMf0tgfAZT31b2h9r1n0YZfvI5/HoosrvlwA4G7aCB3AiWh5r1n0ZszjJvK1WYDzOizy/FD+3kTETNP36Y1er+hJ80Gz+KYrloXplt3y1basXk122r8KraMnzbtDFLBZB0Sr6Jc1dyxGt1g1i35ZczisTWu3XtEva16fL8ZZdasTXTanR2Wy6RqdmhfQNPpwQM3z6Rl92OUDap5Ly+h3OY9p64F8OkY/4PwpNS+gX/TmPqePshTTLvrDLu/SF64V0y36Aefx/30Kd4jK2Q2zYa9M9GFMh/PRCBULmGf6LFiR6LuclnYcGht9VMllpgPH851ViP4w5nzrP3+bSy+M7XVUqMFcAD3Wufnozaf0Nj/12jCMUR+V8SNIzLzx6LtdOpqn7hiJnXVUxGENSJF3w9HvDjiPaYe41CtD2uujEjazIJnsRqPL5AP6cr0zLWNme15Cv+hNmby7Syv7Qn/bOLXTgqRb9EcHXUp+wZ4xt7GOmQqO6Z6ZE/3ek7XEk3tA/rD8qHl3n8uFnZIveW4YFWb3vLR9dvT7a6n7yByWH8niWzyxRVdpGXYMNXsL16rNbAAWCzKjP1g78yBzWHb0/eFuzBPxY9rqNdPIOGen1cf1cf3Ihe1HUKJv3Jp7ucj0Mnd4+ejDjDr6whPfPv26RbLtGUu2R3dwbQKfMeaFSnTFmiJzyE9dffTt02eDXM1Gqz8RY0gT0cO/cDtWAFQa/Z30+9zIIGWih+IIibH4iuu0fkuhZsoclh+RUsv7sbAA52jqojLPFpWeZQ7Lj0ipEzlnOnVgiQ4qUPXZ+wOQcpdsY3HsTk9Qgeqv00nZmzMnYiJsVES9h/bmXuaw/IiUvg1rC3EMop+if7iEQnwF0U/Rv1Yn04mgLdI0VPAQxVhYrjgC0U3B41LO9MiZXawTzRQ8GHksxrL81AXRS/4j0KE4QWJM53K6Kfiww/wc7oTO5TQzGiGHY6etXZt2wdTLIQghhBBC8Ad8qkq/qMyt1gAAAABJRU5ErkJggg==)\n",
        "\n",
        "Curve from the logistic function. [Ref. 1](https://www.javatpoint.com/logistic-regression-in-machine-learning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7_HcN6SI949"
      },
      "source": [
        "\n",
        "In logistic regression, we use a logistic function, also called *sigmoid*, which is a version of the linear function (y = m * x + b) used in linear regression. This function aims to map the predicted values for the probabilities, and any real value is mapped to another value within the range 0 and 1, with no possibility of resulting values greater than 1 or negative. It is interesting to note that for implementing a good Machine Learning model, the selection of this threshold probability that will limit the classifications is a key factor.\n",
        "\n",
        "\n",
        "Assumptions for Logistic Regression:\n",
        "- The dependent variable must be categorical in nature.\n",
        "- The independent variable should not have multi-collinearity.\n",
        "\n",
        "\n",
        "To implement the Logistic Regression, we will follow the steps below:\n",
        "- Data Pre-processing\n",
        "- Fitting Logistic Regression to the Training set\n",
        "- Predicting the test result\n",
        "- Test accuracy (Creation of Confusion matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5XdLhqnshTw"
      },
      "source": [
        "# Conditional (or Binary) Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP940Zufr-gr"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## 1. Data Pre-processing:\n",
        "In this step we will select the dataset that will be used and prepareted.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y41VnNCLDV6f"
      },
      "source": [
        "# Import necessary libraries \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdL63nwPzisP"
      },
      "source": [
        "In the first case, for the binary classification problem we will use a dataset with some numeric values associated to random grades of students in a specific subject. The idea is to conduct an analisys of academic acceptance and classify if they were succesful (1 = positive class) or failed (0 = negative class)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgxPacndMnHq",
        "outputId": "bd8b07bf-953f-429c-f3ca-62ae1f1a55d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "grades = pd.DataFrame({'Student_ID' : pd.Series([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]),\n",
        "                '1st_grade' : pd.Series([7, 4, 5, 6, 6, 8, 2, 4, 3, 9, 4, 5, 6, 1, 2, 4]),\n",
        "               '2nd_grade' : pd.Series([2, 6, 5, 8, 9, 9, 4, 7, 10, 8, 4, 5, 2, 9, 10, 7]),\n",
        "               '3rd_grade' : pd.Series([9, 8, 2, 6, 6, 7, 2, 5, 9, 2, 6, 7, 10, 8, 7, 5]),\n",
        "               'Mean' : pd.Series([6, 6, 4, 6.7, 7, 8, 2.7, 5.3, 7.3, 6.3, 4.7, 5.7, 6, 6, 6.3, 5.3]),\n",
        "               'Situation' : pd.Series([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0]),\n",
        "                })\n",
        "\n",
        "print(grades)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Student_ID  1st_grade  2nd_grade  3rd_grade  Mean  Situation\n",
            "0            0          7          2          9   6.0          1\n",
            "1            1          4          6          8   6.0          1\n",
            "2            2          5          5          2   4.0          0\n",
            "3            3          6          8          6   6.7          1\n",
            "4            4          6          9          6   7.0          1\n",
            "5            5          8          9          7   8.0          1\n",
            "6            6          2          4          2   2.7          0\n",
            "7            7          4          7          5   5.3          0\n",
            "8            8          3         10          9   7.3          1\n",
            "9            9          9          8          2   6.3          1\n",
            "10          10          4          4          6   4.7          0\n",
            "11          11          5          5          7   5.7          0\n",
            "12          12          6          2         10   6.0          1\n",
            "13          13          1          9          8   6.0          1\n",
            "14          14          2         10          7   6.3          1\n",
            "15          15          4          7          5   5.3          0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbOgkgsCKcf3"
      },
      "source": [
        "For better analysis of the model will be used the matplotlib library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcxVpncxKbaI",
        "outputId": "67373e10-679d-4d5c-da18-201355b33a7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.scatter(grades['Mean'], grades['Situation'], marker = '.')\n",
        "plt.ylabel(\"Situation\")\n",
        "plt.xlabel(\"Mean\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnUPnAHNt6Rv",
        "outputId": "d56e25f6-3584-4c5e-86bb-e9d94f97c5d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "#Extracting independent and dependent variables  \n",
        "x= grades.iloc[:, [1,2,3]].values  #The grades of the students are de independent variables\n",
        "y= grades.iloc[:, 5].values  #The gituation of the students are de dependent variables\n",
        "\n",
        "print(x)\n",
        "print(\"----\")\n",
        "print(y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 7  2  9]\n",
            " [ 4  6  8]\n",
            " [ 5  5  2]\n",
            " [ 6  8  6]\n",
            " [ 6  9  6]\n",
            " [ 8  9  7]\n",
            " [ 2  4  2]\n",
            " [ 4  7  5]\n",
            " [ 3 10  9]\n",
            " [ 9  8  2]\n",
            " [ 4  4  6]\n",
            " [ 5  5  7]\n",
            " [ 6  2 10]\n",
            " [ 1  9  8]\n",
            " [ 2 10  7]\n",
            " [ 4  7  5]]\n",
            "----\n",
            "[1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rdcbgd2ySHx",
        "outputId": "8031f38f-cee1-480e-80b9-6682d563b8f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# Splitting the dataset into training and test set.  \n",
        "from sklearn.model_selection import train_test_split  \n",
        "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.4, random_state=0) \n",
        "\n",
        "print(\"FOR TEST SET\")\n",
        "print(x_test)\n",
        "print(\"----\")\n",
        "print(y_test)\n",
        "\n",
        "\n",
        "print(\"\\n FOR TRAINING SET\")\n",
        "print(x_train)\n",
        "print(\"----\")\n",
        "print(y_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOR TEST SET\n",
            "[[ 4  6  8]\n",
            " [ 2  4  2]\n",
            " [ 3 10  9]\n",
            " [ 9  8  2]\n",
            " [ 1  9  8]\n",
            " [ 6  9  6]\n",
            " [ 5  5  2]]\n",
            "----\n",
            "[1 0 1 1 1 1 0]\n",
            "\n",
            " FOR TRAINING SET\n",
            "[[ 2 10  7]\n",
            " [ 4  4  6]\n",
            " [ 4  7  5]\n",
            " [ 4  7  5]\n",
            " [ 5  5  7]\n",
            " [ 6  8  6]\n",
            " [ 7  2  9]\n",
            " [ 8  9  7]\n",
            " [ 6  2 10]]\n",
            "----\n",
            "[1 0 0 0 0 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiimoPPozHmb"
      },
      "source": [
        "In logistic regression, feature scaling needs to be done because we want accurate result of predictions. Here we will only scale the independent variable because dependent variable have only 0 and 1 values. Below is the code for it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdItnd6mzG0H",
        "outputId": "1d86312b-50a4-4660-a927-06281b4c5e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "#feature scaling  \n",
        "from sklearn.preprocessing import StandardScaler    \n",
        "st_x= StandardScaler()    \n",
        "x_train= st_x.fit_transform(x_train)    \n",
        "x_test= st_x.transform(x_test) \n",
        "\n",
        "print(\"FOR TEST SET\")\n",
        "print(x_test)\n",
        "\n",
        "print(\"\\n FOR TRAINING SET\")\n",
        "print(x_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOR TEST SET\n",
            "[[-0.64282435  0.          0.69673301]\n",
            " [-1.79990817 -0.72760688 -3.06562526]\n",
            " [-1.22136626  1.45521375  1.32379273]\n",
            " [ 2.24988521  0.72760688 -3.06562526]\n",
            " [-2.37845008  1.09141031  0.69673301]\n",
            " [ 0.51425948  1.09141031 -0.55738641]\n",
            " [-0.06428243 -0.36380344 -3.06562526]]\n",
            "\n",
            " FOR TRAINING SET\n",
            "[[-1.79990817  1.45521375  0.0696733 ]\n",
            " [-0.64282435 -0.72760688 -0.55738641]\n",
            " [-0.64282435  0.36380344 -1.18444612]\n",
            " [-0.64282435  0.36380344 -1.18444612]\n",
            " [-0.06428243 -0.36380344  0.0696733 ]\n",
            " [ 0.51425948  0.72760688 -0.55738641]\n",
            " [ 1.09280139 -1.45521375  1.32379273]\n",
            " [ 1.6713433   1.09141031  0.0696733 ]\n",
            " [ 0.51425948 -1.45521375  1.95085244]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwh3Cy6SwNeY"
      },
      "source": [
        "## 2. Fitting Logistic Regression to the Training set\n",
        "After importing the class, we will create a classifier object and use it to fit the model to the logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHOTQYyW02YH",
        "outputId": "9bb44a72-ea0d-4441-a4f6-cb3f5fa8e7e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "classifier= LogisticRegression(random_state=0)  \n",
        "classifier.fit(x_train, y_train)  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrGcZrxK1CpU"
      },
      "source": [
        "## 3. Predicting the test set result\n",
        "To predict the result using test set data, it is necessary to creat a vector for test set result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrhUCAcC1gp_",
        "outputId": "16d9b25a-f60d-4061-c06f-a6da749a04e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred= classifier.predict(x_test)\n",
        "\n",
        "print (y_pred)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 1 0 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPYvtQiv5tQi",
        "outputId": "48eb3b94-48e2-468e-bba1-869d908e2283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "classifier.predict_proba(x_train)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.39870144, 0.60129856],\n",
              "       [0.75461927, 0.24538073],\n",
              "       [0.74706136, 0.25293864],\n",
              "       [0.74706136, 0.25293864],\n",
              "       [0.47554552, 0.52445448],\n",
              "       [0.39275574, 0.60724426],\n",
              "       [0.20830481, 0.79169519],\n",
              "       [0.12406493, 0.87593507],\n",
              "       [0.15180446, 0.84819554]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smlmg19Pwxiz"
      },
      "source": [
        "## 4. Test Accuracy\n",
        "\n",
        "One method to discover the model's effectiveness is through the use of the confusion matrix, which represents the produced values of the model used in comparison with the real values, that is, it is possible to know the model's precision knowing how many values were produced correctly. \n",
        "\n",
        "![confusion.png](data:image/png;base64,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)\n",
        "\n",
        "Confusion matrix. [Ref. 2](https://medium.com/@hpsuresh12345/logistic-regression-60694a973bee)\n",
        "\n",
        "\n",
        "The confusion matrix, presented above, also helps us to find the accuracy of the model and avoid overfitting. Each values represents:\n",
        "\n",
        "- In the upper left, we have the true positives (TP), values of the class sought correctly. \n",
        "\n",
        "- In the upper right corner are false positives (FP), which are the values of the class sought that were provided incorrectly. \n",
        "\n",
        "- At the bottom left are the false negatives (FN), the values of the class we are not looking for that were not published correctly. \n",
        "\n",
        "- At the lower right corner, there are the true negatives (TN), values of the class that we are not looking for that were correct. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bo1fNHE1_wR",
        "outputId": "a6bf169d-ab01-4ffd-a119-5660e1a874e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Creating the Confusion matrix to test the performance of the model\n",
        "from sklearn.metrics import confusion_matrix  \n",
        "cm= confusion_matrix(y_test, y_pred)  \n",
        "\n",
        "print(cm)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 0]\n",
            " [1 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TysmVBWRQRHb"
      },
      "source": [
        "From the data generated by the matrix, it is possible to know the percentage of correctness of the model used through the equation:\n",
        "\n",
        "$$Accuracy = Correct Predictions / Observations  = ( TP + TN ) / ( TP + FP + TN + FN )$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avLaFBCR3EEz",
        "outputId": "471cf885-fe15-4790-d3f6-5446ea04a7c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score \n",
        "print (\"Accuracy : \", accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :  0.8571428571428571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stcNaSkRu-qY"
      },
      "source": [
        "# Multiclass Classification\n",
        "\n",
        "In the previous classifications, we worked with two categories or classes of data. With this, we could ask the question: “Did student X pass the test?” The answers “Yes” or “No” could be represented as 1 or 0, but if you need to work with any other question, such as “What is the genre of a particular movie?”, for exemple, there will be many possible answers to this question, so what can be done in this situation? \n",
        "   \n",
        "This is an example of a **Multiclass Classification** problem, there are some methods to solve this problem, one of which is called <ins>one-versus-all</ins> or <ins>one-versus-rest</ins>. The ideia of this strategy aims to reduce the problem of multiclass classification to multiple binary classification problems. \n",
        "\n",
        "The techniques developed based on reducing the multi-class problem into multiple binary problems can also be called problem transformation techniques.One-versus-rest strategy involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. In other words, for this method, we choose a single category as a positive case (1) and the rest of the categories as false (0), in the example previously used for movie genres, we could have as answers: \"horror\", \"action\", \"fiction\" and \"drama\". With that, we can start by classifying “action” as the only positive case (1) and the rest of the genres, negative (0), after that, we will consider “horror” as the only positive case (1) and so on. In this way, we were able to divide the problem into several binary classifications and in each observation made, a probability of the model belonging to each category is generated.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkQo5MaNvVxb"
      },
      "source": [
        "#Final considerations\n",
        "\n",
        "Based on what was previously presented, we were able to identify some interesting points related to *Logistic Regression* [Ref. 2](https://medium.com/@hpsuresh12345/logistic-regression-60694a973bee):\n",
        "\n",
        "**Advantages**:\n",
        "\n",
        "- This technique presented itself very efficient, which does not require too many computational resources;\n",
        "- It’s highly interpretable and doesn’t require input features to be scaled;\n",
        "- It’s also easy to regularize and it outputs well-calibrated predicted probabilities.\n",
        "- Logistic regression does work better when you remove attributes that are unrelated to the output variable as well as attributes that are very similar (correlated) to each other;\n",
        "- Feature Engineering plays an important role in regards to the performance of Logistic and also Linear Regression;\n",
        "- This type of regression can be implemented relatively easy and quick, which make it as good baseline that you can use to measure the performance of other more complex Algorithms.\n",
        "\n",
        "**Disadvantages**:\n",
        "\n",
        "- With this technique we can’t solve non-linear problems since it’s decision surface is linear and can be easily outperformed by other complex and powerful algorithms;\n",
        "- Logistic regression will not perform well with independent variables that are not correlated to the target variable and are very similar or correlated to each other."
      ]
    }
  ]
}